{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf547f6e3ce94d36b0fbdbc4479dfe5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c3c858b51ee484698ae75b60ffd868f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9f2b515f9bd74862abd194f71c03a941",
              "IPY_MODEL_1a441db828844742bbd94c51c9a0f2f5"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "5c3c858b51ee484698ae75b60ffd868f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "9f2b515f9bd74862abd194f71c03a941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b3d4fa6736c6473f9f1ee283a8253c2d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce077ed46e524effb4a2827dd22045dc"
          },
          "model_module_version": "1.5.0"
        },
        "1a441db828844742bbd94c51c9a0f2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5d4cb771e7a0488fafb50ccba3c9a677",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:10&lt;00:00, 15644566.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e32364b3049b429bb256f9dfb67e959f"
          },
          "model_module_version": "1.5.0"
        },
        "b3d4fa6736c6473f9f1ee283a8253c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "ce077ed46e524effb4a2827dd22045dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "5d4cb771e7a0488fafb50ccba3c9a677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "e32364b3049b429bb256f9dfb67e959f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8FVtVyUlMX6"
      },
      "source": [
        "# Assignment 4 -- MOE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOd_cAxplJwl"
      },
      "source": [
        "Please submit this file to Luminus by **23:59 on 11 Mar**. \n",
        "1. Finish 3 tasks according to the instructions. Only change the code in the required area and DO NOT change others or add new code/text snippets.\n",
        "2. Rename this file as \"Student_number.ipynb\". e.g., 'A0000000J.ipynb'. \n",
        "\n",
        "3. Submit the file to /Files/assignments/submission/assignment4.\n",
        "\n",
        "Please follow the instructions strictly, otherwise you might be penalized.\n",
        "\n",
        "If you has any questions, please propose it on Slack, or contact Qin Ziheng(e0823059@u.nus.edu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MdOef4QzmSR"
      },
      "source": [
        "# Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0OOh14MzmSY"
      },
      "source": [
        "In this assignment, we will learn how to implement Sparsely Gated Mixture of Experts by Pytorch proposed in the paper Outrageously Large Neural Networks.\n",
        "\n",
        "If you run the codes on your machine, torch version 1.0.0 is tested and recommended.\n",
        "\n",
        "There are 3 tasks for you in the assignment.\n",
        "\n",
        "\n",
        "1.   finish *combine* function to combine the outputs of experts in the SparseDispatcher\n",
        "2.   implement forward function to get the output in the MOE Network\n",
        "3.   complement the forward, backward, and optimize codes in experiments on cifar10\n",
        "\n",
        "\n",
        "\n",
        "Here are some key components in the workflow:\n",
        "\n",
        "- SparseDispatcher to create inputs for experts and combine the outputs\n",
        "- A sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts\n",
        "- An experimental procedure that test MOE on cifar10\n",
        "\n",
        "More hints may be found in tutorial slides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U7ydc_9zmSb"
      },
      "source": [
        "# MOE Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auefZecRzmSb"
      },
      "source": [
        "Now, letâ€™s define a class MLP class first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPwu8dppzOLk"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.log_soft = nn.LogSoftmax(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.log_soft(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNuHlIgc1_8A"
      },
      "source": [
        "Then we implement the SparseDispatcher class. The purpose of the class ***SparseDispatcher*** is to create input minibatches for the experts and to combine the results of the experts to form a unified output tensor. \n",
        "\n",
        "There are two functions: *dispatch* - take an input Tensor and create input Tensors for each expert. *combine* - take output Tensors from each expert and form a combined output Tensor. Outputs from different experts for the same batch element are summed together, weighted by the provided \"gates\".\n",
        " **The task 1 is to complement the *combine* function.**\n",
        "The class is initialized with a \"gates\" Tensor, which specifies which\n",
        "batch elements go to which experts, and the weights to use when combining\n",
        "the outputs.  Batch element b is sent to expert e iff gates[b, e] != 0.\n",
        "The inputs and outputs are all two-dimensional [batch, depth].\n",
        "Caller is responsible for collapsing additional dimensions prior to\n",
        "calling this class and reshaping the output to the original shape.\n",
        "See common_layers.reshape_like()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRkG8plNzmSc"
      },
      "source": [
        "import torch\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "import numpy as np\n",
        "class SparseDispatcher(object):\n",
        "    \"\"\"Helper for implementing a mixture of experts.\n",
        "    \n",
        "    Example use:\n",
        "    gates: a float32 `Tensor` with shape `[batch_size, num_experts]`\n",
        "    inputs: a float32 `Tensor` with shape `[batch_size, input_size]`\n",
        "    experts: a list of length `num_experts` containing sub-networks.\n",
        " \n",
        "    expert_inputs = sparseDispatcher.dispatch(inputs)\n",
        "    expert_outputs = [experts[i](expert_inputs[i]) for i in range(num_experts)]\n",
        "    \n",
        "    The preceding code sets the output for a particular example b to:\n",
        "    output[b] = Sum_i(gates[b, i] * experts[i](inputs[b]))\n",
        "    This class takes advantage of sparsity in the gate matrix by including in the\n",
        "    `Tensor`s for expert i only the batch elements for which `gates[b, i] > 0`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_experts, gates):\n",
        "        \"\"\"Create a SparseDispatcher.\"\"\"\n",
        "\n",
        "        self._gates = gates\n",
        "        self._num_experts = num_experts\n",
        "        # sort experts\n",
        "        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n",
        "        # drop indices\n",
        "        _, self._expert_index = sorted_experts.split(1, dim=1)\n",
        "        # get according batch index for each expert\n",
        "        self._batch_index = sorted_experts[index_sorted_experts[:, 1],0]\n",
        "        # calculate num samples that each expert gets\n",
        "        self._part_sizes = list((gates > 0).sum(0).numpy())\n",
        "        # expand gates to match with self._batch_index\n",
        "        gates_exp = gates[self._batch_index.flatten()]\n",
        "        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n",
        "\n",
        "    def dispatch(self, inp):\n",
        "        \"\"\"Create one input Tensor for each expert.\n",
        "        The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n",
        "        to the batch elements `b` where `gates[b, i] > 0`.\n",
        "        Args:\n",
        "          inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n",
        "        Returns:\n",
        "          a list of `num_experts` `Tensor`s with shapes\n",
        "            `[expert_batch_size_i, <extra_input_dims>]`.\n",
        "        \"\"\"\n",
        "\n",
        "        # assigns samples to experts whose gate is nonzero\n",
        "\n",
        "        # expand according to batch index so we can just split by _part_sizes\n",
        "        inp_exp = inp[self._batch_index].squeeze(1)\n",
        "        return torch.split(inp_exp, self._part_sizes, dim=0)\n",
        "\n",
        "\n",
        "    def combine(self, expert_out, multiply_by_gates=True):\n",
        "        \"\"\"Sum together the expert output, weighted by the gates.\n",
        "        The slice corresponding to a particular batch element `b` is computed\n",
        "        as the sum over all experts `i` of the expert output, weighted by the\n",
        "        corresponding gate values.  If `multiply_by_gates` is set to False, the\n",
        "        gate values are ignored.\n",
        "        Args:\n",
        "          expert_out: a list of `num_experts` `Tensor`s, each with shape\n",
        "            `[expert_batch_size_i, <extra_output_dims>]`.\n",
        "          multiply_by_gates: a boolean\n",
        "        Returns:\n",
        "          a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n",
        "        \"\"\"\n",
        "        # apply exp to expert outputs, so we are not longer in log space\n",
        "        stitched = torch.cat(expert_out, 0).exp()\n",
        "\n",
        "        if multiply_by_gates:\n",
        "            stitched = stitched.mul(self._nonzero_gates)\n",
        "        zeros = torch.zeros(self._gates.size(0), expert_out[-1].size(1), requires_grad=True)\n",
        "        # combine samples that have been processed by the same k experts\n",
        "        # Task1: code here\n",
        "        # hint: index_add\n",
        "        # hint: variables: _batch_index and stitched\n",
        "        # output varible: combined\n",
        "        \n",
        "        # add eps to all zero values in order to avoid nans when going back to log space\n",
        "        combined[combined == 0] = np.finfo(float).eps\n",
        "        # back to log space\n",
        "        return combined.log()\n",
        "\n",
        "\n",
        "    def expert_to_gates(self):\n",
        "        \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n",
        "        Returns:\n",
        "          a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n",
        "              and shapes `[expert_batch_size_i]`\n",
        "        \"\"\"\n",
        "        # split nonzero gates for each expert\n",
        "        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH-m7amJ18AL"
      },
      "source": [
        "Then, we define and implement our MOE class.\n",
        "**The task 2 is to complement the forward function using the SparseDispatcher class you built.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rz3CxsE3WEF"
      },
      "source": [
        "class MoE(nn.Module):\n",
        "\n",
        "    \"\"\"Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n",
        "    Args:\n",
        "    input_size: integer - size of the input\n",
        "    output_size: integer - size of the input\n",
        "    num_experts: an integer - number of experts\n",
        "    hidden_size: an integer - hidden size of the experts\n",
        "    noisy_gating: a boolean\n",
        "    k: an integer - how many experts to use for each batch element\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_experts, hidden_size, noisy_gating=True, k=4):\n",
        "        super(MoE, self).__init__()\n",
        "        self.noisy_gating = noisy_gating\n",
        "        self.num_experts = num_experts\n",
        "        self.output_size = output_size\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.k = k\n",
        "        # instantiate experts\n",
        "        self.experts = nn.ModuleList([MLP(self.input_size, self.output_size, self.hidden_size) for i in range(self.num_experts)])\n",
        "        self.w_gate = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n",
        "        self.w_noise = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n",
        "\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.softmax = nn.Softmax(1)\n",
        "        self.normal = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
        "\n",
        "        assert(self.k <= self.num_experts)\n",
        "\n",
        "    def cv_squared(self, x):\n",
        "        \"\"\"The squared coefficient of variation of a sample.\n",
        "        Useful as a loss to encourage a positive distribution to be more uniform.\n",
        "        Epsilons added for numerical stability.\n",
        "        Returns 0 for an empty Tensor.\n",
        "        Args:\n",
        "        x: a `Tensor`.\n",
        "        Returns:\n",
        "        a `Scalar`.\n",
        "        \"\"\"\n",
        "        eps = 1e-10\n",
        "        # if only num_experts = 1\n",
        "        if x.shape[0] == 1:\n",
        "            return torch.Tensor([0])\n",
        "        return x.float().var() / (x.float().mean()**2 + eps)\n",
        "\n",
        "\n",
        "    def _gates_to_load(self, gates):\n",
        "        \"\"\"Compute the true load per expert, given the gates.\n",
        "        The load is the number of examples for which the corresponding gate is >0.\n",
        "        Args:\n",
        "        gates: a `Tensor` of shape [batch_size, n]\n",
        "        Returns:\n",
        "        a float32 `Tensor` of shape [n]\n",
        "        \"\"\"\n",
        "        return (gates > 0).sum(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _prob_in_top_k(self, clean_values, noisy_values, noise_stddev, noisy_top_values):\n",
        "        \"\"\"Helper function to NoisyTopKGating.\n",
        "        Computes the probability that value is in top k, given different random noise.\n",
        "        This gives us a way of backpropagating from a loss that balances the number\n",
        "        of times each expert is in the top k experts per example.\n",
        "        In the case of no noise, pass in None for noise_stddev, and the result will\n",
        "        not be differentiable.\n",
        "        Args:\n",
        "        clean_values: a `Tensor` of shape [batch, n].\n",
        "        noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n",
        "          normally distributed noise with standard deviation noise_stddev.\n",
        "        noise_stddev: a `Tensor` of shape [batch, n], or None\n",
        "        noisy_top_values: a `Tensor` of shape [batch, m].\n",
        "           \"values\" Output of tf.top_k(noisy_top_values, m).  m >= k+1\n",
        "        Returns:\n",
        "        a `Tensor` of shape [batch, n].\n",
        "        \"\"\"\n",
        "\n",
        "        batch = clean_values.size(0)\n",
        "        m = noisy_top_values.size(1)\n",
        "        top_values_flat = noisy_top_values.flatten()\n",
        "        threshold_positions_if_in = torch.arange(batch) * m + self.k\n",
        "        threshold_if_in = torch.unsqueeze(torch.gather(top_values_flat, 0, threshold_positions_if_in), 1)\n",
        "        is_in = torch.gt(noisy_values, threshold_if_in)\n",
        "        threshold_positions_if_out = threshold_positions_if_in - 1\n",
        "        threshold_if_out = torch.unsqueeze(torch.gather(top_values_flat,0 , threshold_positions_if_out), 1)\n",
        "        # is each value currently in the top k.\n",
        "        prob_if_in = self.normal.cdf((clean_values - threshold_if_in)/noise_stddev)\n",
        "        prob_if_out = self.normal.cdf((clean_values - threshold_if_out)/noise_stddev)\n",
        "        prob = torch.where(is_in, prob_if_in, prob_if_out)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n",
        "        \"\"\"Noisy top-k gating.\n",
        "          See paper: https://arxiv.org/abs/1701.06538.\n",
        "          Args:\n",
        "            x: input Tensor with shape [batch_size, input_size]\n",
        "            train: a boolean - we only add noise at training time.\n",
        "            noise_epsilon: a float\n",
        "          Returns:\n",
        "            gates: a Tensor with shape [batch_size, num_experts]\n",
        "            load: a Tensor with shape [num_experts]\n",
        "        \"\"\"\n",
        "        clean_logits = x @ self.w_gate\n",
        "        if self.noisy_gating:\n",
        "            raw_noise_stddev = x @ self.w_noise\n",
        "            noise_stddev = ((self.softplus(raw_noise_stddev) + noise_epsilon) * train)\n",
        "            noisy_logits = clean_logits + ( torch.randn_like(clean_logits) * noise_stddev)\n",
        "            logits = noisy_logits\n",
        "        else:\n",
        "            logits = clean_logits\n",
        "\n",
        "        # calculate topk + 1 that will be needed for the noisy gates\n",
        "        top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n",
        "        top_k_logits = top_logits[:, :self.k]\n",
        "        top_k_indices = top_indices[:, :self.k]\n",
        "        top_k_gates = self.softmax(top_k_logits)\n",
        "\n",
        "        zeros = torch.zeros_like(logits, requires_grad=True)\n",
        "        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n",
        "\n",
        "        if self.noisy_gating and self.k < self.num_experts:\n",
        "            load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)\n",
        "        else:\n",
        "            load = self._gates_to_load(gates)\n",
        "        return gates, load\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, train=True, loss_coef=1e-2):\n",
        "        \"\"\"Args:\n",
        "        x: tensor shape [batch_size, input_size]\n",
        "        train: a boolean scalar.\n",
        "        loss_coef: a scalar - multiplier on load-balancing losses\n",
        "\n",
        "        Returns:\n",
        "        y: a tensor with shape [batch_size, output_size].\n",
        "        extra_training_loss: a scalar.  This should be added into the overall\n",
        "        training loss of the model.  The backpropagation of this loss\n",
        "        encourages all experts to be approximately equally used across a batch.\n",
        "        \"\"\"\n",
        "        gates, load = self.noisy_top_k_gating(x, train)\n",
        "        # calculate importance loss\n",
        "        importance = gates.sum(0)\n",
        "        #\n",
        "        loss = self.cv_squared(importance) + self.cv_squared(load)\n",
        "        loss *= loss_coef\n",
        "\n",
        "        # Task2 : Initialize the SparseDispatcher class, dispatch, gate, and combine the experts outputs \n",
        "        # Code Block\n",
        "        # hint: dispatch, combine, and expert_to_gates function in SparseDispatcher\n",
        "        # hint: self.experts[i]\n",
        "        \n",
        "        # y is the combination of expert outputs\n",
        "        return y, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzNG-RD4zmSd"
      },
      "source": [
        "# CIFAR10 Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqnjvvrLzmSd"
      },
      "source": [
        "Now you can use the MOE codes to run CIFAR10 experiments and test your codes. We expect to to achieve a test accuracy of around 39%. **The task 3 is complete the forward+backward+optimization part in the training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nq0jB3-zmSe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "bf547f6e3ce94d36b0fbdbc4479dfe5f",
            "5c3c858b51ee484698ae75b60ffd868f",
            "9f2b515f9bd74862abd194f71c03a941",
            "1a441db828844742bbd94c51c9a0f2f5",
            "b3d4fa6736c6473f9f1ee283a8253c2d",
            "ce077ed46e524effb4a2827dd22045dc",
            "5d4cb771e7a0488fafb50ccba3c9a677",
            "e32364b3049b429bb256f9dfb67e959f"
          ]
        },
        "outputId": "20b87441-d3c6-495f-9844-a43e89a7b0e8"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf547f6e3ce94d36b0fbdbc4479dfe5f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Wftb305k3S"
      },
      "source": [
        "Instantiate the MoE layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKW7crPU5gk7",
        "outputId": "061a1664-ef24-4a2b-802a-6da719b0f430"
      },
      "source": [
        "net = MoE(input_size=3072,output_size= 10, num_experts=10, hidden_size=256, noisy_gating=True, k=4)\n",
        "net = net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        \n",
        "        # Task 3: forward + backward + optimize\n",
        "        # calculate network outputs and aux_loss\n",
        "        # use outputs and labels to calculate CrossEntropyLoss\n",
        "        # calculate total_loss\n",
        "        # backward\n",
        "        # optimize\n",
        "\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs, _ = net(images.view(images.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 2.280\n",
            "[1,   200] loss: 2.226\n",
            "[1,   300] loss: 2.155\n",
            "[1,   400] loss: 2.097\n",
            "[1,   500] loss: 2.032\n",
            "[1,   600] loss: 2.000\n",
            "[1,   700] loss: 1.959\n",
            "[2,   100] loss: 1.892\n",
            "[2,   200] loss: 1.856\n",
            "[2,   300] loss: 1.859\n",
            "[2,   400] loss: 1.823\n",
            "[2,   500] loss: 1.818\n",
            "[2,   600] loss: 1.787\n",
            "[2,   700] loss: 1.791\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 39 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNQFZYUrzmSj"
      },
      "source": [
        "# Further thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQkT6ZKPzmSj"
      },
      "source": [
        "In this assignment, we implement Sparsely-Gated Mixture-of-Experts Layers and conduct experiments in CIFAR10 in Pytorch. Please see \"Outrageously Large Neural Networks\" (https://arxiv.org/abs/1701.06538). \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JSPpjVCzmSj"
      },
      "source": [
        "You can also refer to **Tensorflow** implementation for your reference. ( https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/expert_utils.py)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration Part (0.5 bonus capped at full score)\n",
        "\n",
        "Try to modify one of the MLP (experts) design, gated setting or other way you can think of to enhance this naive implementation. Write a brief report on your finding (either successful or not). You can substitute modules by starting new blocks below."
      ],
      "metadata": {
        "id": "N6NU226VF3Be"
      }
    }
  ]
}
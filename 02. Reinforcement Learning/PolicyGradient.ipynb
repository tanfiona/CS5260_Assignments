{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyGradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 2\n",
        "1. This assignment is due in two weeks, at 23:59 Feb 11th 2022.\n",
        "2. There are two files to submit. Please name your .py and .ipynb file using your student number as Axxxxxx.py, Axxxxxxx.ipynb and submit it to Luminus->assignments->submissions->assignment2"
      ],
      "metadata": {
        "id": "g9iqbJnc_D1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Policy Gradients\n",
        "\n",
        "You will implement the vanilla policy gradients algorithm, also referred to as\n",
        "REINFORCE.\n",
        "\n",
        "## Review\n",
        "\n",
        "In policy gradients, the objective is to learn a parameter $\\theta^*$ that\n",
        "maximizes the following objective:\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta(\\tau)}[R(\\tau)]\n",
        "\\end{equation}\n",
        "\n",
        "where $\\tau = (s_1,a_1,s_2,\\ldots,s_{T-1},a_{T-1},s_T)$ is a *trajectory*\n",
        "(also referred to as an *episode*), and factorizes as\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_\\theta(\\tau) = p(s_1)\\pi_\\theta(a_1|s_1)\\prod_{t=2}^{T} p(s_t|s_{t-1},a_{t-1})\\pi_\\theta(a_t|s_t)\n",
        "\\end{equation}\n",
        "\n",
        "and $R(\\tau)$ denotes the full trajectory reward $R(\\tau) = \\sum_{t=1}^{T}\n",
        "r(s_t,a_t)$ with $r(s_t,a_t)$ the rewards at the individual time steps.\n",
        "\n",
        "In policy gradients, we directly apply the gradient $\\nabla_\\theta$ to\n",
        "$J(\\theta)$. In order to do so, we require samples of trajectories, meaning that\n",
        "we now denote them as $\\tau_i$ for the $i$th trajectory, and have $\\tau_i =\n",
        "(s_{i1},a_{i1},s_{i2},\\ldots,s_{iT})$. When we approximate the gradient with\n",
        "samples, we get:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla_\\theta J(\\theta) &\\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_\\theta \\log \\pi_\\theta(\\tau_i) R(\\tau_i) \\\\\n",
        "&= \\frac{1}{N}\\sum_{i=1}^N \\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right)  \\left( \\sum_{t=1}^{T} r(s_{it},a_{it}) \\right)\n",
        "\\end{align*}\n",
        "\n",
        "Multiplying a discount factor $\\gamma$ to the rewards can be interpreted as\n",
        "encouraging the agent to focus on rewards closer in the future, which can also\n",
        "be thought of as a means for reducing variance (because there are more\n",
        "possible futures further into the future). The discount factor can be\n",
        "incorporated in two ways, from the full trajectory:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "and from the reward to go:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t'=t}^T \\gamma^{t'-t} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "**In this assignment, we only focus on the first version: full tragectory.**\n",
        "\n"
      ],
      "metadata": {
        "id": "lCWlbWAwiOx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradients Implementation\n",
        "\n",
        "\n",
        "**You will need to write code in `PolicyGradient.ipynb`. The places where you need to write code are\n",
        " clearly indicated with the comments `START OF YOUR CODE` and\n",
        "`END OF YOUR CODE`. \n",
        "You do not need to change any other files for this part of the assignment.**\n",
        "\n",
        "The dataflow of the code is structured like this: \n",
        "\n",
        "- Set Up Hyperparameters and environment.\n",
        "- Build a MLP model for policy learning.\n",
        "- Initialize the agent, such as define the policy network and optimizer.\n",
        "- Forward Computation: Sample trajectories by conducting an action given an observation from the environment, and calculate sum of rewards in each trajectory, That includes `sample_action`, `sample_trajectory`, `sample_trajectories` and `sum_of_rewards`.\n",
        "- Backward Computation: Optimize the policy network based on the update rule. That contains `compute_advantage`, `estimate_return`, `get_log_prob` , `update_parameters`.\n",
        "\n",
        "## Problem 1: data sampling\n",
        "\n",
        "You need to implement any parts with a \"Problem 1\" header in the code. Here's what you need to do:\n",
        "\n",
        "- 1. Implement `sample_action`, which samples an action from $\\pi_\\theta(a|s)$. This operation will be called in `sample_trajectories`.\n",
        "- 2. Implement `sample_trajectory`, you need to call `sample_action` to obtain current action.\n",
        "- 3. Implement `sum_of_rewards`, which is the Monte Carlo estimation of the Q function. You need to estimate the q-value of each path and return a single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "\n",
        "## Problem 2: apply policy gradient\n",
        "You only need to implement the parts with the \"Problem 2\" header.\n",
        "\n",
        "- **Estimate return**: in `estimate_return`, normalize the advantages to have a mean of zero and a standard deviation of one.  This is a trick for reducing variance.\n",
        "- Implement `get_log_prob` to obtain $\\log \\pi_\\theta(a_{it}|s_{it})$: Given an action that the agent took in the environment, this computes the log probability of that action under $\\pi_\\theta(a|s)$. This will be used in the parameters update: \n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "- **Update parameters**: In `update_parameters`, using the update operation `optimizer.step()` to update the parameters of the policy. You firstly need to create loss value with the inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "7UzZoFBEnaN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Introduction: \n",
        "\n",
        "\n",
        "##[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/): \n",
        "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077). A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
        "\n",
        "### Observation Space\n",
        "The observation is a `ndarray` with shape `(4,)` where the elements correspond to the following:\n",
        "\n",
        "| Num | Observation           | Min                  | Max                |\n",
        "|-----|-----------------------|----------------------|--------------------|\n",
        "| 0   | Cart Position         | -4.8*                |  4.8*                |\n",
        "| 1   | Cart Velocity         | -Inf                 | Inf                |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°)** | ~ 0.418 rad (24°)** |\n",
        "| 3   | Pole Angular Velocity | -Inf                 | Inf                 |\n",
        "\n",
        "- `*`: the cart x-position can be observed between `(-4.8, 4.8)`, but an episode terminates if the cart leaves the\n",
        "    `(-2.4, 2.4)` range.\n",
        "- `**`: Similarly, the pole angle can be observed between  `(-.418, .418)` radians or precisely **±24°**, but an episode is\n",
        "    terminated if the pole angle is outside the `(-.2095, .2095)` range or precisely **±12°**\n",
        "\n",
        "### Action Space\n",
        "The agent take a 1-element vector for actions.\n",
        "The action space is `(action)` in `[0, 1]`, where `action` is used to push\n",
        "the cart with a fixed amount of force:\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing.\n",
        "This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Rewards\n",
        "Reward is 1 for every step taken, including the termination step.\n",
        "### Starting State\n",
        "All observations are assigned a uniform random value between (-0.05, 0.05).\n",
        "### Episode Termination\n",
        "The episode terminates of one of the following occurs:\n",
        "1. Pole Angle is more than ±12°\n",
        "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
        "3. Episode length is greater than 200. \n"
      ],
      "metadata": {
        "id": "vSvcalq4OcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.10.5"
      ],
      "metadata": {
        "id": "l1gjw8-xs1r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPM8ZfzReLBm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import time\n",
        "import inspect\n",
        "import sys\n",
        "from multiprocessing import Process\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Hyperparameters"
      ],
      "metadata": {
        "id": "XMeurTX_Qnac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'CartPole-v0'\n",
        "# exp_name = 'vpg'\n",
        "render = False\n",
        "animate = render\n",
        "discount = 1.0\n",
        "n_iter = 101\n",
        "batch_size = 1000\n",
        "ep_len = -1.\n",
        "learning_rate = 5e-3\n",
        "reward_to_go = False\n",
        "dont_normalize_advantages = False\n",
        "seed = 1\n",
        "n_experiments = 1\n",
        "max_path_length = ep_len if ep_len > 0 else None\n",
        "min_timesteps_per_batch = batch_size\n",
        "gamma = discount\n",
        "normalize_advantages = not(dont_normalize_advantages)"
      ],
      "metadata": {
        "id": "RWAgpuL5qG5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "bjen3xsdQvaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================================================================#\n",
        "# Set Up Env\n",
        "#========================================================================================#\n",
        "\n",
        "# Make the gym environment\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.seed(seed)\n",
        "\n",
        "# Maximum length for episodes\n",
        "max_path_length = max_path_length or env.spec.max_episode_steps\n",
        "\n",
        "# Is this env continuous, or self.discrete? In this assignment, we only consider discrete action space.\n",
        "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "# Observation and action sizes\n",
        "ob_dim = env.observation_space.shape[0]\n",
        "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]"
      ],
      "metadata": {
        "id": "Q86wu9Q1JP_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a MLP model for policy learning."
      ],
      "metadata": {
        "id": "mMnEZgWfSiR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_actions):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 32)\n",
        "        self.dense2 = nn.Linear(32, 32)\n",
        "        self.dense3 = nn.Linear(32, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.dense1(x))\n",
        "        x = F.tanh(self.dense2(x))\n",
        "        out = F.softmax(self.dense3(x))\n",
        "        return out"
      ],
      "metadata": {
        "id": "FNQgI6V7erIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Initialize Agent\n",
        "    "
      ],
      "metadata": {
        "id": "Ln23veEJLnRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = MLP(input_size=ob_dim, num_actions=ac_dim)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "_IKxBHwBFnDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling"
      ],
      "metadata": {
        "id": "bFVJfx6z-RgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_action(policy_parameters):\n",
        "    \"\"\"\n",
        "    Stochastically sampling from the policy distribution\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "    returns:\n",
        "        sy_sampled_ac: (batch_size,)\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #========================================================================================#\n",
        "    # Stochastically sampling an action from the policy distribution $\\pi_\\theta(a|s)$.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    return sy_sampled_ac"
      ],
      "metadata": {
        "id": "ctKFEkatGfDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectory(env):\n",
        "    ob = env.reset()\n",
        "    obs, acs, rewards = [], [], []\n",
        "    steps = 0\n",
        "    while True:\n",
        "\n",
        "        obs.append(ob)\n",
        "        #====================================================================================#\n",
        "        #                           ----------PROBLEM 1----------\n",
        "        #====================================================================================#\n",
        "        # obtain the action 'ac' for current observation 'ob'\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "        ac = ac.numpy()[0]\n",
        "        acs.append(ac)\n",
        "        ob, rew, done, _ = env.step(ac)\n",
        "        rewards.append(rew)\n",
        "        steps += 1\n",
        "        if done or steps > max_path_length:\n",
        "            break\n",
        "    path = {\"observation\" : np.array(obs, dtype=np.float32),\n",
        "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
        "            \"action\" : np.array(acs, dtype=np.float32)}\n",
        "    return path\n"
      ],
      "metadata": {
        "id": "Z7hSoAK0HP81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_trajectories(itr, env):\n",
        "    \"\"\"Collect paths until we have enough timesteps, as determined by the\n",
        "    length of all paths collected in this batch.\n",
        "    \"\"\"\n",
        "    timesteps_this_batch = 0\n",
        "    paths = []\n",
        "    while True:\n",
        "        path = sample_trajectory(env)\n",
        "        paths.append(path)\n",
        "        timesteps_this_batch += len(path[\"reward\"])\n",
        "        if timesteps_this_batch > min_timesteps_per_batch:\n",
        "            break\n",
        "    return paths, timesteps_this_batch"
      ],
      "metadata": {
        "id": "UdwFJkpbHGmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sum of rewards, we use the total discounted reward summed over entire trajectory (regardless of which time step the Q-value should be for)."
      ],
      "metadata": {
        "id": "-c4Rgb7jC9jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_of_rewards(re_n):\n",
        "    \"\"\" Monte Carlo estimation of the Q function.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        the function sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "    ----------------------------------------------------------------------------------\n",
        "\n",
        "    Your code should construct numpy arrays for Q-values which will be used to compute\n",
        "    advantages.\n",
        "\n",
        "\n",
        "    You will write code for trajectory-based PG: \n",
        "\n",
        "          We use the total discounted reward summed over\n",
        "          entire trajectory (regardless of which time step the Q-value should be for).\n",
        "\n",
        "          For this case, the policy gradient estimator is\n",
        "\n",
        "              E_{tau} [sum_{t=0}^T grad log pi(a_t|s_t) * Ret(tau)]\n",
        "\n",
        "          where\n",
        "\n",
        "              tau=(s_0, a_0, ...) is a trajectory,\n",
        "              Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}.\n",
        "\n",
        "          Thus, you should compute\n",
        "\n",
        "              Q_t = Ret(tau)\n",
        "\n",
        "    Store the Q-values for all timesteps and all trajectories in a variable 'q_n',\n",
        "    like the 'ob_no' and 'ac_na' above.\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 1----------\n",
        "    #====================================================================================#\n",
        "    # q_n: A single vector for the estimated q values whose length is the sum of the lengths of the paths.\n",
        "    # Q-values: Q_t = Ret(tau) = sum_{t'=0}^T gamma^t' r_{t'}. \n",
        "    # Store the Q-values for all timesteps and all trajectories in a variable 'q_n'.\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "    # # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return q_n"
      ],
      "metadata": {
        "id": "7HFdZ45SHm-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Policy Gradient\n",
        "\n",
        "We firstly need to estimate return `estimate_return` and calculate log probability of actions `get_log_prob`. Then we can update parameters based on the rule:\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N\n",
        "\\left( \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta(a_{it}|s_{it}) \\right) \n",
        "\\left( \\sum_{t=1}^T \\gamma^{t-1} r(s_{it},a_{it}) \\right)\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "W86DkWbVgNvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_advantage(ob_no, q_n):\n",
        "  \n",
        "    adv_n = q_n.copy()\n",
        "    return adv_n"
      ],
      "metadata": {
        "id": "3gdKRz2UH6BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_return(ob_no, re_n):\n",
        "    \"\"\" Estimates the returns over a set of trajectories.\n",
        "\n",
        "    let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
        "        sample_trajectories\n",
        "    let num_paths be the number of paths sampled from sample_trajectories\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        re_n: length: num_paths. Each element in re_n is a numpy array\n",
        "            containing the rewards for the particular path\n",
        "\n",
        "    returns:\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "    \"\"\"\n",
        "    q_n = sum_of_rewards(re_n)\n",
        "    adv_n = compute_advantage(ob_no, q_n)\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    # Advantage Normalization\n",
        "    #====================================================================================#\n",
        "    if normalize_advantages:\n",
        "        # On the next line, implement a trick which is known empirically to reduce variance\n",
        "        # in policy gradient methods: normalize adv_n to have mean zero and std=1.\n",
        "        # ------------------------------------------------------------------\n",
        "        # START OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # END OF YOUR CODE\n",
        "        # ------------------------------------------------------------------\n",
        "    return q_n, adv_n"
      ],
      "metadata": {
        "id": "p0bSKK4KICGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_log_prob(policy_parameters, sy_ac_na):\n",
        "    \"\"\"\n",
        "    Computing the log probability of a set of actions that were actually taken according to the policy\n",
        "\n",
        "    arguments:\n",
        "        policy_parameters: logits of a categorical distribution over actions\n",
        "                sy_logits_na: (batch_size, self.ac_dim)\n",
        "\n",
        "        sy_ac_na: (batch_size,)\n",
        "\n",
        "    returns:\n",
        "        sy_logprob_n: (batch_size)\n",
        "\n",
        "    Hint:\n",
        "        For the discrete case, use the log probability under a categorical distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    sy_logits_na = policy_parameters\n",
        "    #========================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #========================================================================================#\n",
        "    # sy_logprob_n = \\sum_{t=1}^T \\log \\pi_\\theta(a_{it}|s_{it})\n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "    return sy_logprob_n"
      ],
      "metadata": {
        "id": "mfS5P1B6Gq5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(ob_no, ac_na, q_n, adv_n):\n",
        "    \"\"\"\n",
        "    Update the parameters of the policy and (possibly) the neural network baseline,\n",
        "    which is trained to approximate the value function.\n",
        "\n",
        "    arguments:\n",
        "        ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
        "        ac_na: shape: (sum_of_path_lengths).\n",
        "        q_n: shape: (sum_of_path_lengths). A single vector for the estimated q values\n",
        "            whose length is the sum of the lengths of the paths\n",
        "        adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
        "            advantages whose length is the sum of the lengths of the paths\n",
        "\n",
        "    returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    #====================================================================================#\n",
        "    #                           ----------PROBLEM 2----------\n",
        "    #====================================================================================#\n",
        "    # Performing the Policy Update based on the current batch of rollouts.\n",
        "    # \n",
        "    # ------------------------------------------------------------------\n",
        "    # START OF YOUR CODE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # END OF YOUR CODE\n",
        "    # ------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "S_xQL3kPINdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop."
      ],
      "metadata": {
        "id": "4zp8VyVQgQ-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running experiment with seed %d'%seed)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "total_timesteps = 0\n",
        "\n",
        "return_data = []\n",
        "\n",
        "for itr in range(n_iter):\n",
        "\n",
        "    paths, timesteps_this_batch = sample_trajectories(itr, env)\n",
        "    total_timesteps += timesteps_this_batch\n",
        "\n",
        "    # Build arrays for observation, action for the policy gradient update by\n",
        "    # concatenating across paths\n",
        "    ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
        "    ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
        "\n",
        "    re_n = [path[\"reward\"] for path in paths]\n",
        "\n",
        "    q_n, adv_n = estimate_return(ob_no, re_n)\n",
        "\n",
        "\n",
        "    update_parameters(ob_no, ac_na, q_n, adv_n)\n",
        "\n",
        "    # Log diagnostics\n",
        "    returns = [path[\"reward\"].sum() for path in paths]\n",
        "\n",
        "    if itr%10 == 0:\n",
        "        print(\"********** Iteration %i ************\"%itr)\n",
        "        ep_lengths = [len(path[\"reward\"]) for path in paths]\n",
        "        print(\"Time: \", time.time() - start)\n",
        "        print(\"Iteration: \", itr)\n",
        "        print(\"AverageReturn: \", np.mean(returns))\n",
        "        print(\"StdReturn: \", np.std(returns))\n",
        "        print(\"MaxReturn: \", np.max(returns))\n",
        "        print(\"MinReturn\", np.min(returns))\n",
        "        print(\"EpLenMean: \", np.mean(ep_lengths))\n",
        "        print(\"EpLenStd: \", np.std(ep_lengths))\n",
        "        print(\"TimestepsThisBatch: \", timesteps_this_batch)\n",
        "        print(\"TimestepsSoFar: \", total_timesteps)\n",
        "    return_data.append(np.mean(returns))"
      ],
      "metadata": {
        "id": "NqS80le8pjC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Average-Return curve.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D4ifa06fgs7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(return_data)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Average Return\")"
      ],
      "metadata": {
        "id": "UvnRU2vT0xIj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}